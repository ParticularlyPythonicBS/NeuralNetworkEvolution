{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.style as mplstyle\n",
    "\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "sns.set_theme(context='paper', style='white', palette='icefire', font='serif',\n",
    "            font_scale=2, color_codes=True, rc={'text.usetex' : True})\n",
    "mplstyle.use('fast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(x):\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(eqx.Module):\n",
    "    weight: jax.Array\n",
    "    bias: jax.Array\n",
    "    activation: callable\n",
    "\n",
    "    def __init__(self, in_features, activation=identity, key=None):\n",
    "        if key is None:\n",
    "            key = jax.random.PRNGKey(0)\n",
    "        w_key, b_key = jax.random.split(key)\n",
    "        self.weight = jax.random.normal(w_key, (in_features,))\n",
    "        self.bias = jax.random.normal(b_key, ())\n",
    "        self.activation = activation\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.activation(jnp.dot(self.weight, x) + self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMLP(eqx.Module):\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, activations, key=None):\n",
    "        if key is None:\n",
    "            key = jax.random.PRNGKey(0)\n",
    "        keys = jax.random.split(key, len(hidden_sizes) + 1)\n",
    "\n",
    "        layers = []\n",
    "        in_features = input_size\n",
    "\n",
    "        for i, out_features in enumerate(hidden_sizes):\n",
    "            layer = [Neuron(in_features, activations[i], key=keys[i]) for _ in range(out_features)]\n",
    "            layers.append(layer)\n",
    "            in_features = out_features\n",
    "\n",
    "        output_layer = [Neuron(in_features, activation=identity, key=keys[-1])]\n",
    "        layers.append(output_layer)\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            # Apply each neuron to each element of x\n",
    "            x = jax.vmap(lambda n, x: n(x), (0, 0))(layer, x)\n",
    "        return x.mean(axis=0)\n",
    "\n",
    "    def add_neuron(self, layer_index, activation=identity, key=None):\n",
    "        if key is None:\n",
    "            key = jax.random.PRNGKey(0)\n",
    "        in_features = self.layers[layer_index][0].weight.shape[0]\n",
    "        new_neuron = Neuron(in_features, activation, key)\n",
    "        self.layers[layer_index].append(new_neuron)\n",
    "\n",
    "        # Adjust the next layer's weight matrix to include the new neuron\n",
    "        if layer_index + 1 < len(self.layers):\n",
    "            for i, next_neuron in enumerate(self.layers[layer_index + 1]):\n",
    "                new_weight = jax.random.normal(key, (1,))\n",
    "                updated_weights = jnp.append(next_neuron.weight, new_weight)\n",
    "                self.layers[layer_index + 1][i] = eqx.tree_at(lambda n: n.weight, next_neuron, updated_weights)\n",
    "\n",
    "    def remove_neuron(self, layer_index, neuron_index):\n",
    "        if len(self.layers[layer_index]) > 0:\n",
    "            del self.layers[layer_index][neuron_index]\n",
    "\n",
    "            # Adjust the next layer's weight matrix to remove the corresponding weight\n",
    "            if layer_index + 1 < len(self.layers):\n",
    "                for i, next_neuron in enumerate(self.layers[layer_index + 1]):\n",
    "                    updated_weights = jnp.delete(next_neuron.weight, neuron_index)\n",
    "                    self.layers[layer_index + 1][i] = eqx.tree_at(lambda n: n.weight, next_neuron, updated_weights)\n",
    "\n",
    "    def get_shape(self):\n",
    "        return [len(layer) for layer in self.layers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_size = 1\n",
    "hidden_sizes = [4, 5]  # Two hidden layers with 4 and 5 neurons respectively\n",
    "output_size = 1\n",
    "activations = [jax.nn.relu, jax.nn.sigmoid]  # Different activation functions for each layer\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "mlp = CustomMLP(input_size, hidden_sizes, output_size, activations, key)\n",
    "\n",
    "# Initialize the optimizer\n",
    "opt = optax.adam(learning_rate=0.01)\n",
    "opt_state = opt.init(eqx.filter(mlp, eqx.is_inexact_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_value_and_grad()\n",
    "def compute_loss(mlp, x, y):\n",
    "    pred = mlp(x)\n",
    "    return jnp.mean((pred - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit()\n",
    "def train_step(mlp, x, y, opt_state, opt_update):\n",
    "    loss, grads = compute_loss(mlp, x, y)\n",
    "    updates, opt_state = opt_update(grads, opt_state)\n",
    "    mlp = eqx.apply_updates(mlp, updates)\n",
    "    return loss, mlp, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example batched training data\n",
    "x = jnp.linspace(-jnp.pi, jnp.pi, 128).reshape(-1, 1)\n",
    "y = jnp.sin(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_optimizer_state(mlp, optimizer):\n",
    "    return optimizer.init(eqx.filter(mlp, eqx.is_inexact_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2, dtype=int32)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_list = [jax.nn.relu, jax.nn.sigmoid, jax.nn.tanh]\n",
    "jax.random.choice(key, jnp.arange(len(activation_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_list = [jax.nn.relu, jax.nn.sigmoid, jax.nn.tanh]\n",
    "num_epochs = 100\n",
    "add_node_every = 1\n",
    "remove_node_every = 1\n",
    "Loss_history = []\n",
    "Node_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "vmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/projects/NeuralNetworkEvolution/env/lib/python3.12/site-packages/jax/_src/api.py:1234\u001b[0m, in \u001b[0;36m_mapped_axis_size.<locals>._get_axis_size\u001b[0;34m(name, shape, axis)\u001b[0m\n\u001b[1;32m   1233\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1234\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1235\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> 2\u001b[0m     loss, mlp, opt_state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     key, add_key, sub_key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key,\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      4\u001b[0m     n_neurons \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(mlp\u001b[38;5;241m.\u001b[39mget_shape())\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[87], line 3\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(mlp, x, y, opt_state, opt_update)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@eqx\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_jit()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(mlp, x, y, opt_state, opt_update):\n\u001b[0;32m----> 3\u001b[0m     loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     updates, opt_state \u001b[38;5;241m=\u001b[39m opt_update(grads, opt_state)\n\u001b[1;32m      5\u001b[0m     mlp \u001b[38;5;241m=\u001b[39m eqx\u001b[38;5;241m.\u001b[39mapply_updates(mlp, updates)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[86], line 3\u001b[0m, in \u001b[0;36mcompute_loss\u001b[0;34m(mlp, x, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;129m@eqx\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_value_and_grad()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_loss\u001b[39m(mlp, x, y):\n\u001b[0;32m----> 3\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mmean((pred \u001b[38;5;241m-\u001b[39m y) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[84], line 25\u001b[0m, in \u001b[0;36mCustomMLP.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;66;03m# Apply each neuron to each element of x\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 5 frame]\u001b[0m\n",
      "File \u001b[0;32m~/projects/NeuralNetworkEvolution/env/lib/python3.12/site-packages/jax/_src/api.py:1238\u001b[0m, in \u001b[0;36m_mapped_axis_size.<locals>._get_axis_size\u001b[0;34m(name, shape, axis)\u001b[0m\n\u001b[1;32m   1236\u001b[0m min_rank \u001b[38;5;241m=\u001b[39m axis \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39maxis\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;66;03m# TODO(mattjj): better error message here\u001b[39;00m\n\u001b[0;32m-> 1238\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m was requested to map its argument along axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich implies that its rank should be at least \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmin_rank\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1241\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut is only \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (its shape is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: vmap was requested to map its argument along axis 0, which implies that its rank should be at least 1, but is only 0 (its shape is ())"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    loss, mlp, opt_state = train_step(mlp, x, y, opt_state, opt.update)\n",
    "    key, add_key, sub_key = jax.random.split(key,3)\n",
    "    n_neurons = sum(mlp.get_shape())\n",
    "    Loss_history.append(loss)\n",
    "    Node_history.append(n_neurons)\n",
    "\n",
    "    # Dynamically add or remove neurons\n",
    "    if (epoch + 1) % add_node_every == 0 and jax.random.uniform(add_key) < 0.05:\n",
    "        add_key, act_key = jax.random.split(add_key)\n",
    "        activation = activation_list[jax.random.choice(key, jnp.arange(len(activation_list)))]\n",
    "        layers = len(mlp.get_shape()) - 1\n",
    "        layer = jax.random.randint(act_key, (1,), 0, layers)[0]\n",
    "        mlp.add_neuron(layer_index=1, activation=activation, key=add_key)\n",
    "        opt_state = initialize_optimizer_state(mlp, opt)\n",
    "        print(f\"Added neuron to hidden layer {layer+1} with activation {activation.__name__}\")\n",
    "        print(mlp.get_shape())\n",
    "    \n",
    "    elif (epoch + 1) % remove_node_every == 0 and jax.random.uniform(sub_key) < 0.05:\n",
    "        layer_key, neuron_key, sub_key = jax.random.split(sub_key,3)\n",
    "        layers = len(mlp.get_shape()) - 1\n",
    "        layer = jax.random.randint(layer_key, (1,), 0, layers)[0]\n",
    "        layer_neurons = len(mlp.layers[layer])\n",
    "        neuron_idx = jax.random.randint(neuron_key, (1,), 0, layer_neurons)[0]\n",
    "        mlp.remove_neuron(layer_index=layer, neuron_index=neuron_idx)\n",
    "        opt_state = initialize_optimizer_state(mlp, opt)\n",
    "        print(f\"Removed neuron from hidden layer {layer+1} at index {neuron_idx}\")\n",
    "        print(mlp.get_shape())\n",
    "\n",
    "    print(f\"Epoch {epoch}, Prediction: {mlp(x)}, Loss: {loss}\")\n",
    "\n",
    "print(\"Final Prediction:\", mlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
