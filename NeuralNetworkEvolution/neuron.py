"""Neuron with activation function included to create neural networks with individual neuron activations"""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_neuron.ipynb.

# %% auto 0
__all__ = ['Neuron']

# %% ../nbs/01_neuron.ipynb 3
import jax
import jax.numpy as jnp
import equinox as eqx

# %% ../nbs/01_neuron.ipynb 4
class Neuron(eqx.Module):
    """
    A simple neuron with a weight vector, bias, and activation function.
    """
    weight: jax.Array
    bias: jax.Array
    activation: callable

    def __init__(self, in_features, activation=jax.nn.relu, bias=False, key=None):
        if key is None:
            key = jax.random.PRNGKey(0)
            key, _ = jax.random.split(key)
        w_key, b_key = jax.random.split(key)
        lim = 1/jnp.sqrt(in_features)
        # self.weight = jax.random.normal(w_key, (in_features,))
        # self.bias = jax.random.normal(b_key, ())
        self.weight = jax.random.uniform(w_key, (in_features,), minval=-lim, maxval=lim)
        if bias:
            self.bias = jax.random.uniform(b_key, (), minval=-lim, maxval=lim)
        else:
            self.bias = None

        self.activation = activation

    def __call__(self, x):
        if self.bias is None:
            return self.activation(jnp.dot(self.weight, x))
        
        return self.activation(jnp.dot(self.weight, x) + self.bias)
    
    def importance(self):
        """
        Returns the importance of the neuron. This is the L2 norm of the weight vector.
        """
        return jnp.linalg.norm(self.weight)/jnp.sqrt(self.weight.size)
