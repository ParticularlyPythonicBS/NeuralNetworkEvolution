{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(eqx.Module):\n",
    "    weight: jax.Array\n",
    "    bias: jax.Array\n",
    "    activation: callable\n",
    "\n",
    "    def __init__(self, in_features, activation=jax.nn.relu, key=None):\n",
    "        if key is None:\n",
    "            key = jax.random.PRNGKey(0)\n",
    "            key, _ = jax.random.split(key)\n",
    "        w_key, b_key = jax.random.split(key)\n",
    "        self.weight = jax.random.normal(w_key, (in_features,))\n",
    "        self.bias = jax.random.normal(b_key, ())\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.activation(jnp.dot(self.weight, x) + self.bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(x):\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMLP(eqx.Module):\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, key=None):\n",
    "        if key is None:\n",
    "            key = jax.random.PRNGKey(0)\n",
    "        keys = jax.random.split(key, len(hidden_sizes) + 1)\n",
    "\n",
    "        layers = []\n",
    "        in_features = input_size\n",
    "\n",
    "        # Create hidden layers\n",
    "        for i, out_features in enumerate(hidden_sizes):\n",
    "            layer = [Neuron(in_features, jax.nn.relu, key=keys[i]) for _ in range(out_features)]\n",
    "            layers.append(layer)\n",
    "            in_features = out_features\n",
    "\n",
    "        # Create output layer\n",
    "        output_layer = [Neuron(in_features, activation=identity, key=keys[-1]) for _ in range(output_size)]\n",
    "        layers.append(output_layer)\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = jnp.array([neuron(x) for neuron in layer])\n",
    "        return x[0]  # Since output layer is a single neuron\n",
    "\n",
    "    def add_neuron(self, layer_index, activation=jax.nn.relu, key=None):\n",
    "        if key is None:\n",
    "            key = jax.random.PRNGKey(0)\n",
    "        in_features = self.layers[layer_index][0].weight.shape[0]\n",
    "        new_neuron = Neuron(in_features, activation, key)\n",
    "        self.layers[layer_index].append(new_neuron)\n",
    "\n",
    "        # Adjust the next layer's weight matrix to include the new neuron\n",
    "        if layer_index + 1 < len(self.layers):\n",
    "            for i, next_neuron in enumerate(self.layers[layer_index + 1]):\n",
    "                new_weight = jax.random.normal(key, (1,))\n",
    "                updated_weights = jnp.append(next_neuron.weight, new_weight)\n",
    "                self.layers[layer_index + 1][i] = eqx.tree_at(lambda n: n.weight, next_neuron, updated_weights)\n",
    "\n",
    "    def remove_neuron(self, layer_index, neuron_index):\n",
    "        if len(self.layers[layer_index]) > 0:\n",
    "            del self.layers[layer_index][neuron_index]\n",
    "        \n",
    "        # Adjust the next layer's weight matrix to remove the corresponding weight\n",
    "        if layer_index + 1 < len(self.layers):\n",
    "            for i, next_neuron in enumerate(self.layers[layer_index + 1]):\n",
    "                updated_weights = jnp.delete(next_neuron.weight, neuron_index)\n",
    "                self.layers[layer_index + 1][i] = eqx.tree_at(lambda n: n.weight, next_neuron, updated_weights)\n",
    "    \n",
    "    def get_shape(self):\n",
    "        return [len(layer) for layer in self.layers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_optimizer_state(mlp, optimizer):\n",
    "    return optimizer.init(eqx.filter(mlp, eqx.is_inexact_array))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_size = 3\n",
    "hidden_sizes = [4, 5]  # Two hidden layers with 4 and 5 neurons respectively\n",
    "output_size = 1\n",
    "\n",
    "key = jax.random.PRNGKey(42)\n",
    "mlp = CustomMLP(input_size, hidden_sizes, output_size, key)\n",
    "opt = optax.adam(learning_rate=1e-2)\n",
    "opt_state = initialize_optimizer_state(mlp, opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 1]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "y = jnp.array([1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_value_and_grad()\n",
    "def compute_loss(mlp, x, y):\n",
    "    pred = mlp(x)\n",
    "    return jnp.mean((pred - y) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@eqx.filter_jit()\n",
    "def train_step(mlp, x, y, opt_state, opt_update):\n",
    "    loss, grads = compute_loss(mlp, x, y)\n",
    "    updates, opt_state = opt_update(grads, opt_state)\n",
    "    mlp = eqx.apply_updates(mlp, updates)\n",
    "    return loss, mlp, opt_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = CustomMLP(input_size, hidden_sizes, output_size, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Prediction: 1.0107297897338867, Loss: 0.0001336823625024408\n",
      "Epoch 1, Prediction: 1.0099060535430908, Loss: 0.00011512838682392612\n",
      "Epoch 2, Prediction: 1.0090949535369873, Loss: 9.812989446800202e-05\n",
      "Epoch 3, Prediction: 1.0083004236221313, Loss: 8.271817932836711e-05\n",
      "Epoch 4, Prediction: 1.00752592086792, Loss: 6.889703217893839e-05\n",
      "Added neuron to hidden layer 2\n",
      "[4, 6, 1]\n",
      "Epoch 5, Prediction: 1.1814756393432617, Loss: 5.6639484682818875e-05\n",
      "Epoch 6, Prediction: 1.152264952659607, Loss: 0.03293340653181076\n",
      "Epoch 7, Prediction: 1.1233584880828857, Loss: 0.02318461611866951\n",
      "Epoch 8, Prediction: 1.09499990940094, Loss: 0.015217316336929798\n",
      "Epoch 9, Prediction: 1.0675103664398193, Loss: 0.00902498234063387\n",
      "Removed neuron from hidden layer 2 at index 0\n",
      "[4, 5, 1]\n",
      "Epoch 10, Prediction: 1.0412989854812622, Loss: 0.004557649604976177\n",
      "Epoch 11, Prediction: 1.0123785734176636, Loss: 0.001705606235191226\n",
      "Epoch 12, Prediction: 0.9876964092254639, Loss: 0.0001532290771137923\n",
      "Epoch 13, Prediction: 0.974411129951477, Loss: 0.00015137834998313338\n",
      "Epoch 14, Prediction: 0.9732325077056885, Loss: 0.0006547902594320476\n",
      "Epoch 15, Prediction: 0.9795669317245483, Loss: 0.0007164986454881728\n",
      "Epoch 16, Prediction: 0.9897061586380005, Loss: 0.00041751028038561344\n",
      "Epoch 17, Prediction: 1.0008538961410522, Loss: 0.00010596316860755906\n",
      "Epoch 18, Prediction: 1.010491967201233, Loss: 7.291386054930626e-07\n",
      "Epoch 19, Prediction: 1.0165894031524658, Loss: 0.0001100813751691021\n",
      "Epoch 20, Prediction: 1.0182960033416748, Loss: 0.0002752082946244627\n",
      "Epoch 21, Prediction: 1.0160058736801147, Loss: 0.00033474373049102724\n",
      "Epoch 22, Prediction: 1.010804295539856, Loss: 0.0002561879809945822\n",
      "Epoch 23, Prediction: 1.0040379762649536, Loss: 0.0001167328009614721\n",
      "Epoch 24, Prediction: 0.997140109539032, Loss: 1.6305251847370528e-05\n",
      "Epoch 25, Prediction: 0.9914737939834595, Loss: 8.178973075700924e-06\n",
      "Epoch 26, Prediction: 0.9880685806274414, Loss: 7.269618799909949e-05\n",
      "Epoch 27, Prediction: 0.9873732328414917, Loss: 0.0001423587673343718\n",
      "Epoch 28, Prediction: 0.9892239570617676, Loss: 0.00015943525067996234\n",
      "Epoch 29, Prediction: 0.9929974675178528, Loss: 0.00011612310481723398\n",
      "Epoch 30, Prediction: 0.9977942705154419, Loss: 4.903546141576953e-05\n",
      "Epoch 31, Prediction: 1.002597689628601, Loss: 4.865242772211786e-06\n",
      "Epoch 32, Prediction: 1.006445288658142, Loss: 6.747991392330732e-06\n",
      "Epoch 33, Prediction: 1.0086264610290527, Loss: 4.154174530413002e-05\n",
      "Epoch 34, Prediction: 1.0088374614715576, Loss: 7.441583147738129e-05\n",
      "Epoch 35, Prediction: 1.007213830947876, Loss: 7.810072565916926e-05\n",
      "Epoch 36, Prediction: 1.0042498111724854, Loss: 5.2039358706679195e-05\n",
      "Epoch 37, Prediction: 1.0006664991378784, Loss: 1.80608949449379e-05\n",
      "Epoch 38, Prediction: 0.9972618818283081, Loss: 4.442210865818197e-07\n",
      "Epoch 39, Prediction: 0.9947478175163269, Loss: 7.497291335312184e-06\n",
      "Epoch 40, Prediction: 0.9935964941978455, Loss: 2.7585419957176782e-05\n",
      "Epoch 41, Prediction: 0.9939446449279785, Loss: 4.1004885133588687e-05\n",
      "Epoch 42, Prediction: 0.9955909252166748, Loss: 3.6667326639872044e-05\n",
      "Epoch 43, Prediction: 0.9980669021606445, Loss: 1.9439939933363348e-05\n",
      "Epoch 44, Prediction: 1.0007562637329102, Loss: 3.7368672565207817e-06\n",
      "Epoch 45, Prediction: 1.0030338764190674, Loss: 5.719348337152041e-07\n",
      "Epoch 46, Prediction: 1.004408836364746, Loss: 9.204406524077058e-06\n",
      "Epoch 47, Prediction: 1.0046342611312866, Loss: 1.943783718161285e-05\n",
      "Epoch 48, Prediction: 1.0037517547607422, Loss: 2.1476376787177287e-05\n",
      "Epoch 49, Prediction: 1.0020604133605957, Loss: 1.4075663784751669e-05\n",
      "Epoch 50, Prediction: 1.0000286102294922, Loss: 4.2453029891476035e-06\n",
      "Epoch 51, Prediction: 0.9981725215911865, Loss: 8.185452315956354e-10\n",
      "Epoch 52, Prediction: 0.9969316720962524, Loss: 3.3396772778360173e-06\n",
      "Epoch 53, Prediction: 0.9965616464614868, Loss: 9.41463622439187e-06\n",
      "Epoch 54, Prediction: 0.9970830082893372, Loss: 1.1822275155282114e-05\n",
      "Epoch 55, Prediction: 0.9982917904853821, Loss: 8.508840437571052e-06\n",
      "Epoch 56, Prediction: 0.9998255372047424, Loss: 2.917979827543604e-06\n",
      "Epoch 57, Prediction: 1.0012651681900024, Loss: 3.043726692908422e-08\n",
      "Epoch 58, Prediction: 1.0022451877593994, Loss: 1.600650534783199e-06\n",
      "Epoch 59, Prediction: 1.0025466680526733, Loss: 5.040868018113542e-06\n",
      "Epoch 60, Prediction: 1.0021463632583618, Loss: 6.485518042609328e-06\n",
      "Epoch 61, Prediction: 1.001209020614624, Loss: 4.6068753363215365e-06\n",
      "Epoch 62, Prediction: 1.0000317096710205, Loss: 1.4617307897424325e-06\n",
      "Epoch 63, Prediction: 0.9989535808563232, Loss: 1.005503236228833e-09\n",
      "Epoch 64, Prediction: 0.9982618093490601, Loss: 1.0949929674097802e-06\n",
      "Epoch 65, Prediction: 0.9981151819229126, Loss: 3.0213068384910002e-06\n",
      "Epoch 66, Prediction: 0.9985086917877197, Loss: 3.552539283191436e-06\n",
      "Epoch 67, Prediction: 0.9992863535881042, Loss: 2.2240001271711662e-06\n",
      "Epoch 68, Prediction: 1.0001952648162842, Loss: 5.09291226080677e-07\n",
      "Epoch 69, Prediction: 1.0009642839431763, Loss: 3.8128348478494445e-08\n",
      "Epoch 70, Prediction: 1.0013835430145264, Loss: 9.298435088567203e-07\n",
      "Epoch 71, Prediction: 1.001360297203064, Loss: 1.9141912162012886e-06\n",
      "Epoch 72, Prediction: 1.0009392499923706, Loss: 1.8504084664527909e-06\n",
      "Epoch 73, Prediction: 1.0002789497375488, Loss: 8.821905339573277e-07\n",
      "Epoch 74, Prediction: 0.9995965957641602, Loss: 7.781295607856009e-08\n",
      "Epoch 75, Prediction: 0.9990991353988647, Loss: 1.6273497749352828e-07\n",
      "Epoch 76, Prediction: 0.9989210963249207, Loss: 8.115570153677254e-07\n",
      "Epoch 77, Prediction: 0.9990895986557007, Loss: 1.1640331649687141e-06\n",
      "Epoch 78, Prediction: 0.9995225667953491, Loss: 8.288305934911477e-07\n",
      "Epoch 79, Prediction: 1.000063419342041, Loss: 2.2794246490320802e-07\n",
      "Epoch 80, Prediction: 1.0005340576171875, Loss: 4.022012944915332e-09\n",
      "Epoch 81, Prediction: 1.000792384147644, Loss: 2.852175384759903e-07\n",
      "Epoch 82, Prediction: 1.0007753372192383, Loss: 6.278726232267218e-07\n",
      "Epoch 83, Prediction: 1.0005121231079102, Loss: 6.011478035361506e-07\n",
      "Epoch 84, Prediction: 1.000109076499939, Loss: 2.6227007765555754e-07\n",
      "Epoch 85, Prediction: 0.9997082948684692, Loss: 1.1897682838934998e-08\n",
      "Epoch 86, Prediction: 0.9994403123855591, Loss: 8.5091883761379e-08\n",
      "Epoch 87, Prediction: 0.9993812441825867, Loss: 3.1325021154771093e-07\n",
      "Epoch 88, Prediction: 0.999532163143158, Loss: 3.828587580301246e-07\n",
      "Epoch 89, Prediction: 0.999824583530426, Loss: 2.188713210671267e-07\n",
      "Epoch 90, Prediction: 1.000148057937622, Loss: 3.077093779779716e-08\n",
      "Epoch 91, Prediction: 1.0003912448883057, Loss: 2.1921152892900864e-08\n",
      "Epoch 92, Prediction: 1.000478744506836, Loss: 1.5307256262531155e-07\n",
      "Epoch 93, Prediction: 1.0003948211669922, Loss: 2.29196302825585e-07\n",
      "Epoch 94, Prediction: 1.0001829862594604, Loss: 1.558837539050728e-07\n",
      "Epoch 95, Prediction: 0.9999280571937561, Loss: 3.348397115132684e-08\n",
      "Epoch 96, Prediction: 0.9997222423553467, Loss: 5.175767370246831e-09\n",
      "Epoch 97, Prediction: 0.9996330142021179, Loss: 7.714930916336016e-08\n",
      "Epoch 98, Prediction: 0.9996813535690308, Loss: 1.3467857229443325e-07\n",
      "Epoch 99, Prediction: 0.9998378157615662, Loss: 1.0153554796943354e-07\n",
      "Final Prediction: 0.9998378\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    loss, mlp, opt_state = train_step(mlp, x, y, opt_state, opt.update)\n",
    "    \n",
    "    # Dynamically add or remove neurons\n",
    "    if epoch == 5:\n",
    "        layer = 1\n",
    "        mlp.add_neuron(layer_index=1, activation=jax.nn.tanh)\n",
    "        opt_state = initialize_optimizer_state(mlp, opt)\n",
    "        print(f\"Added neuron to hidden layer {layer+1}\")\n",
    "        print(mlp.get_shape())\n",
    "    elif epoch == 10:\n",
    "        layer = 1\n",
    "        neuron_idx = 0\n",
    "        mlp.remove_neuron(layer_index=layer, neuron_index=neuron_idx)\n",
    "        opt_state = initialize_optimizer_state(mlp, opt)\n",
    "        print(f\"Removed neuron from hidden layer {layer+1} at index {neuron_idx}\")\n",
    "        print(mlp.get_shape())\n",
    "\n",
    "    print(f\"Epoch {epoch}, Prediction: {mlp(x)}, Loss: {loss}\")\n",
    "\n",
    "print(\"Final Prediction:\", mlp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
