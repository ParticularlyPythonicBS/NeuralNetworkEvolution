# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/Examples/classic_mlp_sine_cos.ipynb.

# %% auto 0
__all__ = ['Model', 'MLP', 'compute_loss', 'train_step']

# %% ../nbs/Examples/classic_mlp_sine_cos.ipynb 4
class Model(eqx.Module):
    layers: list

    def __init__(self, layers):
        self.layers = layers

    def __call__(self, x):
        for layer in self.layers[:-1]:
            x = jax.nn.tanh(layer(x))
        return self.layers[-1](x)

# %% ../nbs/Examples/classic_mlp_sine_cos.ipynb 5
def MLP(layer_sizes, key):
    keys = split(key, len(layer_sizes) - 1)
    layers = [
        eqx.nn.Linear(in_size, out_size, key=k)
        for in_size, out_size, k in zip(layer_sizes[:-1], layer_sizes[1:], keys)
    ]
    return Model(layers)

# %% ../nbs/Examples/classic_mlp_sine_cos.ipynb 6
@eqx.filter_value_and_grad()
def compute_loss(params, x, y):
    preds = jax.vmap(params)(x)
    mse = jnp.mean((preds - y) ** 2)
    nrmse = jnp.sqrt(mse) / jnp.std(y)
    return mse

# %% ../nbs/Examples/classic_mlp_sine_cos.ipynb 7
@eqx.filter_jit
def train_step(params, x, y, opt_state, opt_update):
    loss, grads = compute_loss(params, x, y)
    updates, opt_state = opt_update(grads, opt_state)
    params = eqx.apply_updates(params, updates)
    return loss, params, opt_state
