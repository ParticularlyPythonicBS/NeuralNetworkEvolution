# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/prototyping_node_changes.ipynb.

# %% auto 0
__all__ = ['Model', 'MLP', 'where_weight', 'where_bias', 'add_node', 'remove_node', 'compute_loss', 'train_step']

# %% ../nbs/prototyping_node_changes.ipynb 2
import jax
import jax.numpy as jnp
import numpy as np
import optax
import equinox as eqx
from jax import random
from jax.random import PRNGKey, split

import matplotlib.pyplot as plt
import matplotlib.cm as cm
import matplotlib.colors as colors
import matplotlib.style as mplstyle

import seaborn as sns

# %% ../nbs/prototyping_node_changes.ipynb 5
class Model(eqx.Module):
    layers: list

    def __init__(self, layers):
        self.layers = layers

    def __call__(self, x):
        for layer in self.layers[:-1]:
            x = jax.nn.relu(layer(x))
        return self.layers[-1](x)

# %% ../nbs/prototyping_node_changes.ipynb 6
def MLP(layer_sizes, key):
    keys = split(key, len(layer_sizes) - 1)
    layers = [
        eqx.nn.Linear(in_size, out_size, key=k)
        for in_size, out_size, k in zip(layer_sizes[:-1], layer_sizes[1:], keys)
    ]
    return Model(layers)

# %% ../nbs/prototyping_node_changes.ipynb 10
# getters for pytree manipulation
def where_weight(linear):
    return linear.weight


def where_bias(linear):
    return linear.bias

# %% ../nbs/prototyping_node_changes.ipynb 11
def add_node(mlp, key):
    new_layers = []

    # first layer
    initial_layer = mlp.layers[0]
    out_features, in_features = initial_layer.weight.shape
    new_inital_shape = (out_features + 1, in_features)
    new_initial_weight = jnp.resize(initial_layer.weight, new_inital_shape)
    new_initial_bias = jnp.resize(initial_layer.bias, (new_inital_shape[0],))
    new_initial_layer = eqx.nn.Linear(new_inital_shape[1], new_inital_shape[0], key=key)
    new_initial_layer = eqx.tree_at(where_weight, new_initial_layer, new_initial_weight)
    new_initial_layer = eqx.tree_at(where_bias, new_initial_layer, new_initial_bias)
    new_layers.append(new_initial_layer)

    # hidden layer(s)
    for i, layer in enumerate(mlp.layers[1:-1]):
        out_features, in_features = layer.weight.shape
        new_shape = (out_features + 1, in_features + 1)
        new_weight = jnp.resize(layer.weight, new_shape)
        new_bias = jnp.resize(layer.bias, (new_shape[0],))
        new_layer = eqx.nn.Linear(new_shape[1], new_shape[0], key=key)
        new_layer = eqx.tree_at(where_weight, new_layer, new_weight)
        new_layer = eqx.tree_at(where_bias, new_layer, new_bias)
        new_layers.append(new_layer)

    # final layer
    final_layer = mlp.layers[-1]
    out_features, in_features = final_layer.weight.shape
    new_final_shape = (out_features, in_features + 1)
    new_final_weight = jnp.resize(final_layer.weight, new_final_shape)
    new_final_bias = jnp.resize(final_layer.bias, (new_final_shape[0],))
    new_final_layer = eqx.nn.Linear(new_final_shape[1], new_final_shape[0], key=key)
    new_final_layer = eqx.tree_at(where_weight, new_final_layer, new_final_weight)
    new_final_layer = eqx.tree_at(where_bias, new_final_layer, new_final_bias)
    new_layers.append(new_final_layer)

    return Model(new_layers)

# %% ../nbs/prototyping_node_changes.ipynb 14
def remove_node(mlp, key):
    new_layers = []

    # first layer
    initial_layer = mlp.layers[0]
    out_features, in_features = initial_layer.weight.shape
    new_inital_shape = (out_features - 1, in_features)
    new_initial_weight = jnp.resize(initial_layer.weight, new_inital_shape)
    new_initial_bias = jnp.resize(initial_layer.bias, (new_inital_shape[0],))
    new_initial_layer = eqx.nn.Linear(new_inital_shape[1], new_inital_shape[0], key=key)
    new_initial_layer = eqx.tree_at(where_weight, new_initial_layer, new_initial_weight)
    new_initial_layer = eqx.tree_at(where_bias, new_initial_layer, new_initial_bias)
    new_layers.append(new_initial_layer)

    # hidden layer(s)
    for i, layer in enumerate(mlp.layers[1:-1]):
        out_features, in_features = layer.weight.shape
        new_shape = (out_features - 1, in_features - 1)
        new_weight = jnp.resize(layer.weight, new_shape)
        new_bias = jnp.resize(layer.bias, (new_shape[0],))
        new_layer = eqx.nn.Linear(new_shape[1], new_shape[0], key=key)
        new_layer = eqx.tree_at(where_weight, new_layer, new_weight)
        new_layer = eqx.tree_at(where_bias, new_layer, new_bias)
        new_layers.append(new_layer)

    # final layer
    final_layer = mlp.layers[-1]
    out_features, in_features = final_layer.weight.shape
    new_final_shape = (out_features, in_features - 1)
    new_final_weight = jnp.resize(final_layer.weight, new_final_shape)
    new_final_bias = jnp.resize(final_layer.bias, (new_final_shape[0],))
    new_final_layer = eqx.nn.Linear(new_final_shape[1], new_final_shape[0], key=key)
    new_final_layer = eqx.tree_at(where_weight, new_final_layer, new_final_weight)
    new_final_layer = eqx.tree_at(where_bias, new_final_layer, new_final_bias)
    new_layers.append(new_final_layer)

    return Model(new_layers)

# %% ../nbs/prototyping_node_changes.ipynb 17
@eqx.filter_value_and_grad()
def compute_loss(params, x, y):
    preds = jax.vmap(params)(x)
    return jnp.mean((preds - y) ** 2)

# %% ../nbs/prototyping_node_changes.ipynb 18
@eqx.filter_jit
def train_step(params, x, y, opt_state, opt_update):
    loss, grads = compute_loss(params, x, y)
    updates, opt_state = opt_update(grads, opt_state)
    params = eqx.apply_updates(params, updates)
    return loss, params, opt_state
