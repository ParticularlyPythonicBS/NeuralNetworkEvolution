{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP\n",
    "> Create a multilayer perceptron using the Neuron class.\n",
    "> The mlp has functionality to add or remove nodes during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "\n",
    "from NeuralNetworkEvolution.neuron import Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def identity(x):\n",
    "    \"\"\"\n",
    "    Identity activation function\n",
    "    \"\"\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMLP(eqx.Module):\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, input_size, hidden_sizes, output_size, key=None):\n",
    "        if key is None:\n",
    "            key = jax.random.PRNGKey(0)\n",
    "        keys = jax.random.split(key, len(hidden_sizes) + 1)\n",
    "        act_key = jax.random.split(keys[-1], 1)[0]\n",
    "        activation_list = [jax.nn.relu, jax.nn.sigmoid, jax.nn.tanh]\n",
    "        layers = []\n",
    "        in_features = input_size\n",
    "\n",
    "        # Create hidden layers\n",
    "        for i, out_features in enumerate(hidden_sizes):\n",
    "            layer = [Neuron(in_features, activation_list[0], key=keys[i]) for _ in range(out_features)]\n",
    "            layers.append(layer)\n",
    "            in_features = out_features\n",
    "\n",
    "        # Create output layer\n",
    "        output_layer = [Neuron(in_features, activation=identity, key=keys[-1]) for _ in range(output_size)]\n",
    "        layers.append(output_layer)\n",
    "\n",
    "        self.layers = layers\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = jnp.array([neuron(x) for neuron in layer])\n",
    "        return x[0]  # Since output layer is a single neuron\n",
    "\n",
    "    def add_neuron(self, layer_index, activation=jax.nn.relu, key=None):\n",
    "        if key is None:\n",
    "            key = jax.random.PRNGKey(0)\n",
    "        in_features = self.layers[layer_index][0].weight.shape[0]\n",
    "        new_neuron = Neuron(in_features, activation, key)\n",
    "        self.layers[layer_index].append(new_neuron)\n",
    "\n",
    "        # Adjust the next layer's weight matrix to include the new neuron\n",
    "        if layer_index + 1 < len(self.layers):\n",
    "            for i, next_neuron in enumerate(self.layers[layer_index + 1]):\n",
    "                new_weight = jax.random.normal(key, (1,))\n",
    "                updated_weights = jnp.append(next_neuron.weight, new_weight)\n",
    "                self.layers[layer_index + 1][i] = eqx.tree_at(lambda n: n.weight, next_neuron, updated_weights)\n",
    "\n",
    "    def remove_neuron(self, layer_index, neuron_index):\n",
    "        if len(self.layers[layer_index]) > 0:\n",
    "            del self.layers[layer_index][neuron_index]\n",
    "        \n",
    "        # Adjust the next layer's weight matrix to remove the corresponding weight\n",
    "        if layer_index + 1 < len(self.layers):\n",
    "            for i, next_neuron in enumerate(self.layers[layer_index + 1]):\n",
    "                updated_weights = jnp.delete(next_neuron.weight, neuron_index)\n",
    "                self.layers[layer_index + 1][i] = eqx.tree_at(lambda n: n.weight, next_neuron, updated_weights)\n",
    "    \n",
    "    def get_shape(self):\n",
    "        return [len(layer) for layer in self.layers]\n",
    "\n",
    "    def least_important_neuron(self):\n",
    "        all_importances = []\n",
    "        layer_sizes = []\n",
    "        for layer in self.layers:\n",
    "            importances = [n.importance() for n in layer]\n",
    "            all_importances.append(jnp.array(importances).flatten())  # Flatten the importances\n",
    "            layer_sizes.append(len(layer))\n",
    "\n",
    "        all_importances = jnp.concatenate(all_importances)\n",
    "        sorted_indices = jnp.argsort(all_importances)\n",
    "\n",
    "        # Locate the layer and neuron index\n",
    "        cum_neurons = jnp.cumsum(jnp.array(layer_sizes))\n",
    "        for min_importance_index in sorted_indices:\n",
    "            layer_index = jnp.searchsorted(cum_neurons, min_importance_index, side=\"right\")\n",
    "            neuron_index = min_importance_index - (cum_neurons[layer_index - 1] if layer_index > 0 else 0)\n",
    "            if neuron_index != len(self.layers[layer_index]) - 1:  # If the neuron is not the last one of its layer\n",
    "                return layer_index, neuron_index\n",
    "        \n",
    "        raise ValueError(\"All neurons are the last ones of their layers\")\n",
    "\n",
    "    def most_important_layer(self):\n",
    "        # Calculate the total importance of each layer\n",
    "        layer_importances = [jnp.sum(jnp.array([n.importance() for n in layer])) for layer in self.layers[:-1]]\n",
    "        most_important_layer_index = jnp.argmax(jnp.array(layer_importances))  # Convert to Jax array\n",
    "\n",
    "        return most_important_layer_index\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
